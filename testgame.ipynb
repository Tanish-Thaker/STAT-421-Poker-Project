{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "29bf86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import WesBot as wb\n",
    "import HandOddsCalcWes as hoc\n",
    "\n",
    "import texasholdem as th\n",
    "import texasholdem.evaluator as eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf5b082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Round:\n",
    "  def __init__(self):\n",
    "    self.decisions = []\n",
    "    self.outcome = int(0)\n",
    "  \n",
    "  def add_decision(self, decision:list):\n",
    "    self.decisions.append(decision)\n",
    "  \n",
    "  def close_round(self, outcome:int):\n",
    "    self.outcome = outcome\n",
    "  \n",
    "  def get_decisions(self):\n",
    "    return self.decisions\n",
    "  \n",
    "  def get_one_decision(self, index:int):\n",
    "    return self.decisions[index]\n",
    "  \n",
    "  def get_outcome(self):\n",
    "    return self.outcome\n",
    "  \n",
    "  def copy(self):\n",
    "    output = Round()\n",
    "    for decision in self.decisions:\n",
    "      output.add_decision(decision)\n",
    "    output.close_round(self.outcome)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c3423fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smort_decision(our_bot:PokerBot, game:th.TexasHoldEm)\n",
    "# actually make a goddamn decision using my scuffed bot thingy, and record\n",
    "# all the necessary information for Shro' to use in his thing\n",
    "# INPUTS:\n",
    "#   our_bot: a PokerBot object, supposed to be intelligent or something\n",
    "#   game:    a TexasHoldEm game object that we're playing\n",
    "# OUTPUT:\n",
    "#   a list containing:\n",
    "#     [hand_phase:str,\n",
    "#      my_hand:list of th.Card,\n",
    "#      board:list of th.Card,\n",
    "#      EV:float,\n",
    "#      my_chips_betting:int,\n",
    "#      their_chips_betting:int,\n",
    "#      my_decision:Decision]\n",
    "# SIDE EFFECT:\n",
    "#   makes the decision in the TexasHoldEm game object passed in\n",
    "def smort_decision(our_bot:wb.PokerBot, game:th.TexasHoldEm):\n",
    "  # get information regarding our hand, the game board, our win prob,\n",
    "  # their chips bet, and our EV\n",
    "  win_prob, my_hand_odds = \\\n",
    "    hoc.estimate_win_and_hand_probs(game, game.current_player, 2, 1000)\n",
    "  loss_prob = 1 - win_prob\n",
    "  my_chips_betting = game.player_bet_amount(0) + game.chips_to_call(0)\n",
    "  their_chips_betting = game.player_bet_amount(1)\n",
    "  EV = win_prob*their_chips_betting - loss_prob*my_chips_betting\n",
    "  # get features for Shro'\n",
    "  my_decision = our_bot.make_decision(EV, my_hand_odds, game)\n",
    "  my_hand = game.get_hand(game.current_player)\n",
    "  board = []\n",
    "  for card in game.board:\n",
    "    board.append(card)\n",
    "  # record hand phase\n",
    "  hand_phase = game.hand_phase.name\n",
    "  # actually make the decision\n",
    "  if (my_decision.type == \"RAISE\"):\n",
    "    game.take_action(th.ActionType.RAISE, my_decision.size)\n",
    "  elif (my_decision.type == \"CALL/CHECK\"):\n",
    "    if (game.validate_move(action = th.ActionType.CALL)):\n",
    "      game.take_action(th.ActionType.CALL)\n",
    "    else:\n",
    "      game.take_action(th.ActionType.CHECK)\n",
    "  elif (my_decision.type == \"FOLD\"):\n",
    "    game.take_action(th.ActionType.FOLD)\n",
    "  else:\n",
    "    game.take_action(th.ActionType.ALL_IN)\n",
    "  output = [hand_phase, my_hand, board, EV, my_chips_betting, their_chips_betting, my_decision]\n",
    "  return output\n",
    "\n",
    "# baby_decision(game:th.TexasHoldEm)\n",
    "# make a random decision for the opponent, just to get someone to play against\n",
    "# INPUTS:\n",
    "#   game: a th.TexasHoldEm object that we're playing\n",
    "# SIDE EFFECT:\n",
    "#   makes the random decision in the game object passed in\n",
    "def baby_decision(game:th.TexasHoldEm):\n",
    "  # opponent makes random decision (reused baby code)\n",
    "  babys_decision = np.random.choice(3)\n",
    "  if (babys_decision == 0):\n",
    "    # baby will call/check if possible\n",
    "    if (game.validate_move(action = th.ActionType.CALL) or\n",
    "        game.validate_move(action = th.ActionType.CHECK)):\n",
    "      decision = wb.Decision(\"CALL/CHECK\")\n",
    "    elif (game.validate_move(action = th.ActionType.ALL_IN)):\n",
    "      decision = wb.Decision(\"ALLIN\")\n",
    "    else:\n",
    "      decision = wb.Decision(\"FOLD\")\n",
    "  elif (babys_decision == 1):\n",
    "    # baby will fold\n",
    "    decision = wb.Decision(\"FOLD\")\n",
    "  else:\n",
    "    # baby will raise if possible\n",
    "    min_raise = game.get_available_moves().raise_range.start\n",
    "    max_raise = int(np.min([game.players[game.current_player].chips,\n",
    "                            game.get_available_moves().raise_range.stop]))\n",
    "    if (min_raise <= max_raise and\n",
    "        game.validate_move(action = th.ActionType.RAISE, value = min_raise) and\n",
    "        game.validate_move(action = th.ActionType.RAISE, value = max_raise)):\n",
    "      decision = \\\n",
    "        wb.Decision(\"RAISE\", int(np.random.uniform(min_raise, max_raise)))\n",
    "    elif (game.validate_move(action = th.ActionType.CALL) or\n",
    "          game.validate_move(action = th.ActionType.CHECK)):\n",
    "      decision = wb.Decision(\"CALL/CHECK\")\n",
    "    elif (game.validate_move(action = th.ActionType.ALL_IN)):\n",
    "      decision = wb.Decision(\"ALLIN\")\n",
    "    else:\n",
    "      decision = wb.Decision(\"FOLD\")\n",
    "  # actually make the decision\n",
    "  if (decision.type == \"RAISE\"):\n",
    "    game.take_action(th.ActionType.RAISE, decision.size)\n",
    "  elif (decision.type == \"CALL/CHECK\"):\n",
    "    if (game.validate_move(action = th.ActionType.CALL)):\n",
    "      game.take_action(th.ActionType.CALL)\n",
    "    else:\n",
    "      game.take_action(th.ActionType.CHECK)\n",
    "  elif (decision.type == \"FOLD\"):\n",
    "    game.take_action(th.ActionType.FOLD)\n",
    "    return 0\n",
    "  else:\n",
    "    game.take_action(th.ActionType.ALL_IN)\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "76429652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 26.11it/s]\n"
     ]
    }
   ],
   "source": [
    "game = th.TexasHoldEm(buyin=500, big_blind=5, small_blind=2, max_players=2)\n",
    "game.start_hand()\n",
    "current_round = Round()\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "our_bot = wb.PokerBot(k=10, EV_weight=10.0, maturity=epochs/4)\n",
    "\n",
    "\n",
    "\n",
    "rounds_list = [] # a list of Round objects\n",
    "\n",
    "for i in trange(epochs):\n",
    "\n",
    "  if (game.is_hand_running()):\n",
    "    # control flow time! we are player 0.\n",
    "    if (game.current_player == 0):\n",
    "      # make decision and record decision\n",
    "      our_decision = smort_decision(our_bot, game)\n",
    "      current_round.add_decision(our_decision)\n",
    "      if (our_decision[6].type == \"FOLD\"):\n",
    "        who_won = 1\n",
    "    else:\n",
    "      # the opponent (who is baby) makes a decision\n",
    "      who_won = baby_decision(game)\n",
    "    \n",
    "    # get the hypothetical winner (in case this is the showdown)\n",
    "\n",
    "  # ensure a game and hand is running\n",
    "  if (not game.is_game_running()):\n",
    "    game = th.TexasHoldEm(buyin=500, big_blind=5, small_blind=2, max_players=2)\n",
    "    game.start_hand()\n",
    "    current_round = Round()\n",
    "  # start a new hand if needed\n",
    "  if (not game.is_hand_running()):\n",
    "    current_round.close_round(game._get_last_pot().amount*((-1)**who_won))\n",
    "    rounds_list.append(current_round.copy())\n",
    "    game.start_hand()\n",
    "    current_round = Round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f92ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [decisions, outcomes]\n",
      "Index: []\n",
      "Total Earnings: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with decisions and outcomes\n",
    "data = {\n",
    "    \"decisions\": [round_obj.get_decisions() for round_obj in rounds_list],\n",
    "    \"outcomes\": [round_obj.get_outcome() for round_obj in rounds_list]\n",
    "}\n",
    "decisions_outcomes_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(decisions_outcomes_df)\n",
    "\n",
    "# Calculate total earnings\n",
    "total_earnings = decisions_outcomes_df['outcomes'].sum()\n",
    "print(f\"Total Earnings: {total_earnings}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fb82d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome: 61\n",
      "  Decision: {'hand_phase': 'PREFLOP', 'my_hand': [Card(\"Ah\"), Card(\"5h\")], 'board': [], 'EV': 1.28, 'my_chips_betting': 5, 'their_chips_betting': 5, 'my_decision': <WesBot.Decision object at 0x00000199B5A6EA50>}\n"
     ]
    }
   ],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, decision=None, outcome=None):\n",
    "        self.decision = decision\n",
    "        self.outcome = outcome\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.outcome is not None:\n",
    "            return f\"Outcome: {self.outcome}\"\n",
    "        return f\"Decision: {self.decision}\"\n",
    "\n",
    "\n",
    "def format_decision(decision):\n",
    "    return {\n",
    "        \"hand_phase\": decision[0],\n",
    "        \"my_hand\": decision[1],\n",
    "        \"board\": decision[2],\n",
    "        \"EV\": round(float(decision[3]), 4),\n",
    "        \"my_chips_betting\": decision[4],\n",
    "        \"their_chips_betting\": decision[5],\n",
    "        \"my_decision\": decision[6]\n",
    "    }\n",
    "\n",
    "def build_decision_tree_with_format(row):\n",
    "    # Start with the outcome as the root node\n",
    "    root = DecisionNode(outcome=row['outcomes'])\n",
    "    current_node = root\n",
    "\n",
    "    # Traverse the decisions in order\n",
    "    for decision in row['decisions']:\n",
    "        formatted_decision = format_decision(decision)\n",
    "        new_node = DecisionNode(decision=formatted_decision)\n",
    "        current_node.add_child(new_node)\n",
    "        current_node = new_node  # Move to the new node for the next decision\n",
    "\n",
    "    return root\n",
    "\n",
    "# Build decision trees with formatted decisions for all rows in the DataFrame\n",
    "formatted_decision_trees = [build_decision_tree_with_format(row) for _, row in decisions_outcomes_df.iterrows()]\n",
    "\n",
    "def print_tree(node, level=0):\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}{node}\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, level + 1)\n",
    "\n",
    "# Print the entire tree for the first formatted decision tree\n",
    "print_tree(formatted_decision_trees[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80c280ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1415 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 988         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006877939 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000136   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.91e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 2.33e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 870          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014071108 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00271      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+06     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 2.49e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 839           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00079387845 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0.00294       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.28e+06      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00114      |\n",
      "|    value_loss           | 2.32e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 830          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002505324 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00367      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+06     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000496    |\n",
      "|    value_loss           | 2.53e+06     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gymnasium import spaces  # Ensure gymnasium is used for spaces\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom environment for reinforcement learning\n",
    "from gymnasium import Env\n",
    "\n",
    "class PokerEnv(Env):  # Inherit from gymnasium.Env\n",
    "    def __init__(self, rounds_df):\n",
    "        self.rounds_df = rounds_df\n",
    "        self.current_index = 0\n",
    "        self.action_space = spaces.Discrete(3)  # Example: 3 actions (fold, call, raise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)  # Example state size\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Example implementation: return a dummy observation\n",
    "        # Replace this with actual logic to extract meaningful observations from rounds_df\n",
    "        if self.current_index < len(self.rounds_df):\n",
    "            # Extract the current round's decisions and outcome\n",
    "            current_round = self.rounds_df.iloc[self.current_index]\n",
    "            decisions = current_round['decisions']\n",
    "            \n",
    "            # Initialize the observation array\n",
    "            observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "            \n",
    "            # Analyze each decision to extract meaningful features\n",
    "            for i, decision in enumerate(decisions):\n",
    "                if i < len(observation) - 2:  # Ensure we don't exceed observation size\n",
    "                    # Example: Use EV (expected value) as a feature\n",
    "                    observation[i + 2] = decision[3]  # Assuming EV is at index 3 in the decision\n",
    "            outcome = current_round['outcome']\n",
    "            \n",
    "            # Create a meaningful observation based on decisions and outcome\n",
    "            # For simplicity, we use the length of decisions and outcome as features\n",
    "            observation[0] = len(decisions)  # Number of decisions in the round\n",
    "            observation[1] = outcome  # Outcome of the round\n",
    "            \n",
    "            # Additional features can be added here based on decisions\n",
    "            return observation\n",
    "        else:\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_index = 0\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)  # Set the random seed if provided\n",
    "        return self._get_observation(), {}  # Gymnasium requires reset to return a tuple (obs, info)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_index < len(self.rounds_df):\n",
    "            reward = self.rounds_df.iloc[self.current_index]['outcome']\n",
    "        else:\n",
    "            reward = 0  # Return a default reward if out of bounds\n",
    "        self.current_index += 1  # Increment the index\n",
    "        done = self.current_index >= len(self.rounds_df)  # Define `done` correctly\n",
    "        truncated = False  # Add a placeholder for truncated (not used in this case)\n",
    "        info = {}  # Additional information can be added here\n",
    "\n",
    "        # Ensure the step method returns exactly five values\n",
    "        observation = self._get_observation()\n",
    "        return observation, reward, done, truncated, info\n",
    "# Wrap the environment\n",
    "env = DummyVecEnv([lambda: PokerEnv(rounds_df)])\n",
    "\n",
    "# Create and train the PPO model\n",
    "model1 = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model1.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model1.save(\"poker_rl_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "100c1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Player 0 has state IN cannot CALL",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m     game.take_action(th.ActionType.FOLD)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mActionType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCALL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action == \u001b[32m2\u001b[39m:\n\u001b[32m     37\u001b[39m     min_raise = game.get_available_moves().raise_range.start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ssjed\\OneDrive\\Documents\\GitHub\\STAT-421-Poker-Project\\.venv\\Lib\\site-packages\\texasholdem\\game\\game.py:1134\u001b[39m, in \u001b[36mTexasHoldEm.take_action\u001b[39m\u001b[34m(self, action_type, value, total)\u001b[39m\n\u001b[32m   1125\u001b[39m     warnings.warn(\n\u001b[32m   1126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe :code:`value` has been renamed to :code:`total` and will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbe redefined in 1.0.0. Currently, :code:`value` and :code:`total` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1130\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1131\u001b[39m     )\n\u001b[32m   1132\u001b[39m     total = value\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrows\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28mself\u001b[39m._action = (action_type, total)\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ssjed\\OneDrive\\Documents\\GitHub\\STAT-421-Poker-Project\\.venv\\Lib\\site-packages\\texasholdem\\util\\functions.py:37\u001b[39m, in \u001b[36mcheck_raise.<locals>.decorator.<locals>.inner\u001b[39m\u001b[34m(throws, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m ret, msg = func(*args, **kwargs)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret \u001b[38;5;129;01mand\u001b[39;00m throws:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_type(msg)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mValueError\u001b[39m: Player 0 has state IN cannot CALL"
     ]
    }
   ],
   "source": [
    "# Initialize metrics for tracking total earnings and win rate\n",
    "import math\n",
    "metrics = {\n",
    "    \"total_earnings\": 0,\n",
    "    \"model_win_rate\": 0.0\n",
    "}\n",
    "\n",
    "model_wins = 0  # Counter for model's wins\n",
    "total_games = 0  # Counter for total games played\n",
    "\n",
    "epochs = 1000  # Number of epochs for simulation\n",
    "rounds_list = []  # List to store rounds\n",
    "\n",
    "# Simulate games between the RL model and the smart bot\n",
    "for epoch in trange(epochs):\n",
    "    # Ensure a game and hand are running\n",
    "    game = th.TexasHoldEm(buyin=500, big_blind=5, small_blind=2, max_players=2)\n",
    "    game.start_hand()\n",
    "    while not game.is_hand_running():\n",
    "        game.start_hand()\n",
    "\n",
    "    current_round = Round()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if game.current_player == 0:\n",
    "            # RL model's turn\n",
    "            obs = env.reset()\n",
    "            action, _ = model1.predict(obs, deterministic=True)\n",
    "            if random.random() == 0:\n",
    "                game.take_action(th.ActionType.ALL_IN)\n",
    "            elif action == 0:\n",
    "                game.take_action(th.ActionType.FOLD)\n",
    "            elif action == 1:\n",
    "                game.take_action(th.ActionType.CALL)\n",
    "            elif action == 2:\n",
    "                min_raise = game.get_available_moves().raise_range.start\n",
    "                max_raise = int(np.min([game.players[game.current_player].chips,\n",
    "                                        game.get_available_moves().raise_range.stop]))\n",
    "                if min_raise > max_raise or min_raise <= 0:\n",
    "\n",
    "                    # Fallback to a valid action if min_raise is invalid\n",
    "                    if game.validate_move(action=th.ActionType.CALL):\n",
    "                        game.take_action(th.ActionType.CALL)\n",
    "                    elif game.validate_move(action=th.ActionType.CHECK):\n",
    "                        game.take_action(th.ActionType.CHECK)\n",
    "                    else:\n",
    "                        game.take_action(th.ActionType.FOLD)\n",
    "                else:\n",
    "                    game.take_action(th.ActionType.RAISE, min_raise)\n",
    "            else:\n",
    "                game.take_action(th.ActionType.ALL_IN)\n",
    "        else:\n",
    "            # Smart bot's turn\n",
    "            # Smart bot decision\n",
    "            smort_decision(our_bot, game)\n",
    "            who_won = 0 if game.current_player == 1 else 1  # Determine the winner based on the current player\n",
    "\n",
    "        if not game.is_hand_running():\n",
    "            done = True\n",
    "    outcome = game._get_last_pot().amount * (-1 if who_won == 1 else 1)\n",
    "    # Record the outcome\n",
    "    outcome = game._get_last_pot().amount * ((-1) ** who_won)\n",
    "    current_round.close_round(outcome)\n",
    "    rounds_list.append(current_round.copy())\n",
    "\n",
    "    # Update metrics\n",
    "    total_games += 1\n",
    "    if who_won == 0:\n",
    "        model_wins += 1\n",
    "    metrics[\"total_earnings\"] += outcome\n",
    "    metrics[\"model_win_rate\"] = model_wins / total_games\n",
    "\n",
    "# Print the final metrics\n",
    "print(f\"Final Metrics:\")\n",
    "print(f\"Total games played: {total_games}\")\n",
    "print(f\"Model wins: {model_wins}\")\n",
    "print(f\"Total earnings: {metrics['total_earnings']}\")\n",
    "print(f\"Model win rate: {metrics['model_win_rate']:.2f}\")\n",
    "print(f\"Model earnings per game: {metrics['total_earnings'] / total_games:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -1.15e+03 |\n",
      "|    value_loss         | 1.25e+06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 325      |\n",
      "|    value_loss         | 2.52e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 569       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -433      |\n",
      "|    value_loss         | 1.85e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.32e+03 |\n",
      "|    value_loss         | 1.51e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 589       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -1.09e+03 |\n",
      "|    value_loss         | 1.1e+06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 594      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 53.7     |\n",
      "|    value_loss         | 6.13e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 594      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 416      |\n",
      "|    value_loss         | 3.77e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.16e+03 |\n",
      "|    value_loss         | 1.47e+06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 289      |\n",
      "|    value_loss         | 3.44e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 448       |\n",
      "|    value_loss         | 4.64e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -901     |\n",
      "|    value_loss         | 7.36e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 608      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.07e+03 |\n",
      "|    value_loss         | 1e+06    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 535      |\n",
      "|    value_loss         | 2.38e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 602      |\n",
      "|    value_loss         | 3.94e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -163     |\n",
      "|    value_loss         | 2.33e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -376      |\n",
      "|    value_loss         | 1.86e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -671     |\n",
      "|    value_loss         | 8.75e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 611      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -567     |\n",
      "|    value_loss         | 3.5e+05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 610      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -44.7    |\n",
      "|    value_loss         | 6.61e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -7.15e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -575      |\n",
      "|    value_loss         | 2.73e+05  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Create and train the A2C model\n",
    "a2c_model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "a2c_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the new model\n",
    "a2c_model.save(\"poker_a2c_model\")\n",
    "\n",
    "# Replace the existing model with the new A2C model for evaluation\n",
    "model = a2c_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d6895",
   "metadata": {},
   "source": [
    "# AFTER THIS POINT, NOTHING IS IN USE AND ONLY REMAINS FOR REFERENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFLOP\n"
     ]
    }
   ],
   "source": [
    "print(game.hand_phase.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(game.current_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01752d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Card(\"3c\"), Card(\"4d\")] [] 0.402 5 -155.862\n"
     ]
    }
   ],
   "source": [
    "# get information regarding our hand, the game board, our win prob, their chips bet, and our EV\n",
    "win_prob, my_hand_odds = hoc.estimate_win_and_hand_probs(game, 0, 2, 1000)\n",
    "loss_prob = 1 - win_prob\n",
    "my_chips_betting = game.player_bet_amount(0) + game.chips_to_call(0)\n",
    "their_chips_betting = game.player_bet_amount(1)\n",
    "EV = win_prob*their_chips_betting - loss_prob*my_chips_betting\n",
    "print(game.get_hand(0), game.board, win_prob, their_chips_betting, EV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afcb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n"
     ]
    }
   ],
   "source": [
    "# get decision\n",
    "my_decision = our_bot.make_decision(EV, my_hand_odds, game)\n",
    "print(my_decision.type, my_decision.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually make decision\n",
    "if (my_decision.type == \"RAISE\"):\n",
    "  game.take_action(th.ActionType.RAISE, my_decision.size)\n",
    "elif (my_decision.type == \"CALL/CHECK\"):\n",
    "  if (game.validate_move(action = th.ActionType.CALL)):\n",
    "    game.take_action(th.ActionType.CALL)\n",
    "  else:\n",
    "    game.take_action(th.ActionType.CHECK)\n",
    "elif (my_decision.type == \"FOLD\"):\n",
    "  game.take_action(th.ActionType.FOLD)\n",
    "else:\n",
    "  game.take_action(th.ActionType.ALL_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL/CHECK 0\n"
     ]
    }
   ],
   "source": [
    "# opponent makes random decision (reused baby code)\n",
    "babys_decision = np.random.choice(3)\n",
    "if (babys_decision == 0):\n",
    "  # baby will call/check if possible\n",
    "  if (game.validate_move(action = th.ActionType.CALL) or\n",
    "      game.validate_move(action = th.ActionType.CHECK)):\n",
    "    decision = wb.Decision(\"CALL/CHECK\")\n",
    "  elif (game.validate_move(game.current_player, th.ActionType.ALL_IN)):\n",
    "    decision = wb.Decision(\"ALLIN\")\n",
    "  else:\n",
    "    decision = wb.Decision(\"FOLD\")\n",
    "elif (babys_decision == 1):\n",
    "  # baby will fold\n",
    "  decision = wb.Decision(\"FOLD\")\n",
    "else:\n",
    "  # baby will raise if possible\n",
    "  min_raise = game.get_available_moves().raise_range[0]\n",
    "  max_raise = int(np.min([game.players[game.current_player].chips,\n",
    "                          game.get_available_moves().raise_range[-1]]))\n",
    "  if (min_raise <= max_raise and\n",
    "      game.validate_move(action = th.ActionType.RAISE, value = min_raise) and\n",
    "      game.validate_move(action = th.ActionType.RAISE, value = max_raise)):\n",
    "    decision = \\\n",
    "      wb.Decision(\"RAISE\", int(np.random.uniform(min_raise, max_raise)))\n",
    "  elif (game.validate_move(action = th.ActionType.CALL) or\n",
    "        game.validate_move(action = th.ActionType.CHECK)):\n",
    "    decision = wb.Decision(\"CALL/CHECK\")\n",
    "  elif (game.validate_move(action = th.ActionType.ALL_IN)):\n",
    "    decision = wb.Decision(\"ALLIN\")\n",
    "  else:\n",
    "    decision = wb.Decision(\"FOLD\")\n",
    "print(decision.type, decision.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8d729",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No hand is running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     game.take_action(th.ActionType.CALL)\n\u001b[32m      7\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mActionType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHECK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (decision.type == \u001b[33m\"\u001b[39m\u001b[33mFOLD\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     10\u001b[39m   game.take_action(th.ActionType.FOLD)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ssjed\\OneDrive\\Documents\\GitHub\\STAT-421-Poker-Project\\.venv\\Lib\\site-packages\\texasholdem\\game\\game.py:1117\u001b[39m, in \u001b[36mTexasHoldEm.take_action\u001b[39m\u001b[34m(self, action_type, value, total)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1103\u001b[39m \u001b[33;03mThe current player takes the specified action.\u001b[39;00m\n\u001b[32m   1104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m \n\u001b[32m   1115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_hand_running():\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo hand is running\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;129;01mand\u001b[39;00m value:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGot arguments for both total and value. Expected only one.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1122\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No hand is running"
     ]
    }
   ],
   "source": [
    "# actually make decision\n",
    "if (decision.type == \"RAISE\"):\n",
    "  game.take_action(th.ActionType.RAISE, decision.size)\n",
    "elif (decision.type == \"CALL/CHECK\"):\n",
    "  if (game.validate_move(action = th.ActionType.CALL)):\n",
    "    game.take_action(th.ActionType.CALL)\n",
    "  else:\n",
    "    game.take_action(th.ActionType.CHECK)\n",
    "elif (decision.type == \"FOLD\"):\n",
    "  game.take_action(th.ActionType.FOLD)\n",
    "else:\n",
    "  game.take_action(th.ActionType.ALL_IN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
